# llm-pollute
 
token-decode.py 从llm的tokenizer中提取制作vocab文件。
tokenfreq.py    从vocab文件中分析中文token出现的频率。
./Token-Frquencies  各llm的vocab分析结果
./Polluted-Tokens   人工标注污染token
annotation.txt  人工标注规范
models.csv  目前支持的GPT外的llm