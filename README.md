# llm-pollute
 
token-decode.py 从llm的tokenizer中提取制作vocab文件。/n
tokenfreq.py    从vocab文件中分析中文token出现的频率。/n
./Token-Frquencies  各llm的vocab分析结果/n
./Polluted-Tokens   人工标注污染token/n
annotation.txt  人工标注规范/n
models.csv  目前支持的GPT外的llm/n